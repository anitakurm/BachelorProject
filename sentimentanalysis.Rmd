---
title: "loadsentiment"
author: "Anita Kurm"
date: "November 20, 2018"
output: html_document
---


Sentiment data arrived
```{r}
pacman::p_load(ggplot2,dplyr,lme4)

#merge data 6th to 16th of February to then find sentiment scores
filenames<-list.files(recursive = FALSE)
alldata<-NULL
alldata <- data.frame(Date=as.Date(character()),
                 ID=character(),
                 Text=character(),
                 Language=character())
colnames(alldata)<-c("Datetime","ID","Text","Language")

for (csvfile in filenames){
  d<-read.csv(csvfile)
  alldata<-rbind(alldata,d)
}


#try load sentiment data
d<-read.csv("bigdata1to5sentiment.csv")

#filter round 2
hashtagd<-dplyr::filter(alldata, grepl('firearm|#gun|#guns|#gundebate|gun debate|#2A|2ndamendment|guncontrol|gun control|gunsense|gun sense|gunban|gun ban|gunsafety|gun safety|gunrights|gun rights|#NRA|2AShallNotBeInfringed|#shooting', Text))


#let's save it to run for sentiment analysis
write.csv(hashtagd, 'tryingout.csv')

#download the same thing but with sentiment scores
sentiment<-read.csv('C:/Users/JARVIS/Desktop/Uni/Thesis/data scraping/tryingout10to17.csv')

sentiment$Datetime<-as.character.Date(sentiment$Datetime)
#remove +0000
sentiment$Datetime<-sub('\\+0000', '', sentiment$Datetime) # Changes only the 1st pattern match per string
sentiment$Datetime<- as.POSIXct(sentiment$Datetime, format="%a %b %d %H:%M:%S %Y") #format="%A %M %d %H:%M:%S %Y"


sentiment$day<-as.factor(format(sentiment$Datetime, "%a %b %d"))
sentiment$hour<-as.factor(format(sentiment$Datetime, "%a %b %d %H"))

#find the mean per hour
statshour<-sentiment %>%
  group_by(hour) %>%
  summarise(meanH=mean(Compound), sdH=sd(Compound), minH=min(Compound),maxH=max(Compound), varH=var(Compound), countH=n())

statsday <- sentiment %>%
  group_by(day) %>%
  summarise(meanD=mean(Compound), sdD=sd(Compound), minD=min(Compound),maxD=max(Compound), varD=var(Compound), countD=n())

merged<-merge(sentiment,statsday)
merged<-merge(merged,statshour)

#dplyr library; removing repeating tweets
uniquemerged<- merged %>% 
    group_by(Tweet) %>%
    slice(1L)

```

Some visualizations
```{r}
#compound scores through the time
ggplot(merged, aes(x = Datetime, y = Compound))+
  geom_smooth() +
  geom_point()+
  theme_minimal()+
  ggtitle("Compound scores plotted against timeline")

#compound scores through the time
ggplot(uniquemerged, aes(x = Datetime, y = Compound))+
  geom_smooth() +
  #geom_point()+
  theme_minimal()+
  ggtitle("Compound sentiment scores plotted against timeline, no repetitions")


#Positive and negative scores through the time
ggplot(uniquemerged, aes(x = Datetime))+
  geom_smooth(aes(y=Positive, color='Positive')) +
  geom_smooth(aes(y=Negative, color='Negative')) +
  #geom_point()+
  theme_minimal()+
  ggtitle("Sentiment scores plotted against timeline, no repetitions")


#compound score distribution
ggplot(uniquemerged, aes(x = Compound))+
  geom_density() + 
  theme_minimal()+
  ggtitle("Distribution of compound sentiment scores")

#compound score distribution
ggplot(uniquehashtagd, aes(x = Compound))+
  geom_density() + 
  theme_minimal()+
  ggtitle("Distribution of compound scores, less repetitions")


#mean compound score distribution
ggplot(merged, aes(x = mean))+
  geom_density() + 
  theme_minimal()+
  ggtitle("Distribution of mean compound scores, less repetitions")


#plot the mean
ggplot(merged, aes(x=Datetime2, y=mean))+
  geom_smooth()+
  theme_minimal()+
  ggtitle("Mean compound scores over time")

#plot distribution of tweets over time
ggplot(merged, aes(x=Datetime2, y=COUNT))+
  geom_point()+
  geom_smooth()+
  theme_minimal()+
  ggtitle("Number of tweets over time")


```

Get the data
```{r}

uniquemerged$event <- as.POSIXct("Wed Feb 14 19:21", format="%a %b %d %H:%M")
uniquemerged$proximity<-difftime(uniquemerged$Datetime,uniquemerged$event,units="hours")

write.csv(uniquemerged, "modellingdata.csv")


```

